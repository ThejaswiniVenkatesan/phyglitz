{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86ea900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       INCIDENT N°     PROTECTIONS  \\\n",
      "0        212472461             NaN   \n",
      "1        212472456             NaN   \n",
      "2        212467991             NaN   \n",
      "3        212468029             NaN   \n",
      "4        212465847             NaN   \n",
      "...            ...             ...   \n",
      "10665    128458648             NaN   \n",
      "10666    128458565             NaN   \n",
      "10667    128441430             NaN   \n",
      "10668    128407887  LUP Q000331834   \n",
      "10669    128405012             NaN   \n",
      "\n",
      "                                      ASSIGNED TO TOPICS  \\\n",
      "0                                                    NaN   \n",
      "1                                                    NaN   \n",
      "2                                                    NaN   \n",
      "3                                                    NaN   \n",
      "4                                                    NaN   \n",
      "...                                                  ...   \n",
      "10665                                                NaN   \n",
      "10666                                     SFE-B002222259   \n",
      "10667                                                NaN   \n",
      "10668  T00002959 - BBA - Wiper not working properly &...   \n",
      "10669                                                NaN   \n",
      "\n",
      "      TOPICS OF THE OPERATION STATE  TRANSFERED FROM  TRANSFERRED TO  \\\n",
      "0                         NaN     -              NaN             NaN   \n",
      "1                         NaN     -              NaN             NaN   \n",
      "2                         NaN     -              NaN             NaN   \n",
      "3                         NaN     -              NaN             NaN   \n",
      "4                         NaN     -              NaN             NaN   \n",
      "...                       ...   ...              ...             ...   \n",
      "10665                     NaN     -              NaN             NaN   \n",
      "10666                     NaN     -              NaN             NaN   \n",
      "10667                     NaN     -              NaN             NaN   \n",
      "10668                     NaN     -              NaN             NaN   \n",
      "10669                     NaN     -              NaN             NaN   \n",
      "\n",
      "       Dummy VIN                                         GI COMMENT TAGS  ...  \\\n",
      "0              1                                                NaN  NaN  ...   \n",
      "1              2                                                NaN  NaN  ...   \n",
      "2              3                                                NaN  NaN  ...   \n",
      "3              4                                                NaN  NaN  ...   \n",
      "4              5                                                NaN  NaN  ...   \n",
      "...          ...                                                ...  ...  ...   \n",
      "10665       6202                                                NaN  NaN  ...   \n",
      "10666       7474                                                NaN  NaN  ...   \n",
      "10667       7815                                                NaN  NaN  ...   \n",
      "10668       7816  04-Apr-2023 - BBA - Wipernot working properly ...  RTI  ...   \n",
      "10669       7817                                                NaN  NaN  ...   \n",
      "\n",
      "      FIRST DETECTION LAST DETECTION LAST UPDATE OF DETECTION PILOTE  \\\n",
      "0                 NaN            NaN                             NaN   \n",
      "1                 NaN            NaN                             NaN   \n",
      "2                 NaN            NaN                             NaN   \n",
      "3                 NaN            NaN                             NaN   \n",
      "4                 NaN            NaN                             NaN   \n",
      "...               ...            ...                             ...   \n",
      "10665      04/04/2023     01/05/2023                      04/04/2023   \n",
      "10666      02/04/2023     04/04/2023                      04/04/2023   \n",
      "10667             NaN            NaN                             NaN   \n",
      "10668      04/04/2023     12/04/2023                      29/09/2023   \n",
      "10669             NaN            NaN                             NaN   \n",
      "\n",
      "      LAST UPDATE OF INCIDENT PILOTE                           KEYWORDS  \\\n",
      "0                                NaN                                NaN   \n",
      "1                                NaN                                NaN   \n",
      "2                                NaN                                NaN   \n",
      "3                                NaN                                NaN   \n",
      "4                                NaN                                NaN   \n",
      "...                              ...                                ...   \n",
      "10665                            NaN  acceleration reverse direction EN   \n",
      "10666                            NaN                    wheel locked EN   \n",
      "10667                            NaN                                NaN   \n",
      "10668                            NaN  not switching working switches EN   \n",
      "10669                            NaN                                NaN   \n",
      "\n",
      "                                         DETECTION QUERY source creation date  \\\n",
      "0                                                    NaN                  NaT   \n",
      "1                                                    NaN                  NaT   \n",
      "2                                                    NaN                  NaT   \n",
      "3                                                    NaN                  NaT   \n",
      "4                                                    NaN                  NaT   \n",
      "...                                                  ...                  ...   \n",
      "10665  RTI - EICps Déficience dans la motricité / Tra...           2023-04-01   \n",
      "10666  RTI - EICps Déficience dans la motricité / Tra...                  NaT   \n",
      "10667                                                NaN                  NaT   \n",
      "10668  RTI - EICps Mauvaise signalisation / Bad signa...           2023-03-28   \n",
      "10669                                                NaN           2023-04-11   \n",
      "\n",
      "         year  month   day  \n",
      "0         NaN    NaN   NaN  \n",
      "1         NaN    NaN   NaN  \n",
      "2         NaN    NaN   NaN  \n",
      "3         NaN    NaN   NaN  \n",
      "4         NaN    NaN   NaN  \n",
      "...       ...    ...   ...  \n",
      "10665  2023.0    4.0   1.0  \n",
      "10666     NaN    NaN   NaN  \n",
      "10667     NaN    NaN   NaN  \n",
      "10668  2023.0    3.0  28.0  \n",
      "10669  2023.0    4.0  11.0  \n",
      "\n",
      "[10670 rows x 141 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from Excel file with header starting from the second row\n",
    "df = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Assuming your date column is named 'source creation date'\n",
    "# Convert the 'source creation date' column to datetime format\n",
    "df['source creation date'] = pd.to_datetime(df['SOURCE CREATION DATE'], format='%d/%m/%Y')\n",
    "\n",
    "# Extract year, month, and day into separate columns\n",
    "df['year'] = df['source creation date'].dt.year\n",
    "df['month'] = df['source creation date'].dt.month\n",
    "df['day'] = df['source creation date'].dt.day\n",
    "\n",
    "# Print the DataFrame with separated columns\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fe7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['INCIDENT N°', 'PROTECTIONS', 'ASSIGNED TO TOPICS',\n",
      "       'TOPICS OF THE OPERATION', 'STATE', 'TRANSFERED FROM', 'TRANSFERRED TO',\n",
      "       'Dummy VIN', 'GI COMMENT', 'TAGS',\n",
      "       ...\n",
      "       'FIRST DETECTION', 'LAST DETECTION', 'LAST UPDATE OF DETECTION PILOTE',\n",
      "       'LAST UPDATE OF INCIDENT PILOTE', 'KEYWORDS', 'DETECTION QUERY',\n",
      "       'source creation date', 'year', 'month', 'day'],\n",
      "      dtype='object', length=141)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a3ba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       INCIDENT N°     PROTECTIONS  \\\n",
      "0        212472461             NaN   \n",
      "1        212472456             NaN   \n",
      "2        212467991             NaN   \n",
      "3        212468029             NaN   \n",
      "4        212465847             NaN   \n",
      "...            ...             ...   \n",
      "10665    128458648             NaN   \n",
      "10666    128458565             NaN   \n",
      "10667    128441430             NaN   \n",
      "10668    128407887  LUP Q000331834   \n",
      "10669    128405012             NaN   \n",
      "\n",
      "                                      ASSIGNED TO TOPICS  \\\n",
      "0                                                    NaN   \n",
      "1                                                    NaN   \n",
      "2                                                    NaN   \n",
      "3                                                    NaN   \n",
      "4                                                    NaN   \n",
      "...                                                  ...   \n",
      "10665                                                NaN   \n",
      "10666                                     SFE-B002222259   \n",
      "10667                                                NaN   \n",
      "10668  T00002959 - BBA - Wiper not working properly &...   \n",
      "10669                                                NaN   \n",
      "\n",
      "      TOPICS OF THE OPERATION STATE  TRANSFERED FROM  TRANSFERRED TO  \\\n",
      "0                         NaN     -              NaN             NaN   \n",
      "1                         NaN     -              NaN             NaN   \n",
      "2                         NaN     -              NaN             NaN   \n",
      "3                         NaN     -              NaN             NaN   \n",
      "4                         NaN     -              NaN             NaN   \n",
      "...                       ...   ...              ...             ...   \n",
      "10665                     NaN     -              NaN             NaN   \n",
      "10666                     NaN     -              NaN             NaN   \n",
      "10667                     NaN     -              NaN             NaN   \n",
      "10668                     NaN     -              NaN             NaN   \n",
      "10669                     NaN     -              NaN             NaN   \n",
      "\n",
      "       Dummy VIN                                         GI COMMENT TAGS  ...  \\\n",
      "0              1                                                NaN  NaN  ...   \n",
      "1              2                                                NaN  NaN  ...   \n",
      "2              3                                                NaN  NaN  ...   \n",
      "3              4                                                NaN  NaN  ...   \n",
      "4              5                                                NaN  NaN  ...   \n",
      "...          ...                                                ...  ...  ...   \n",
      "10665       6202                                                NaN  NaN  ...   \n",
      "10666       7474                                                NaN  NaN  ...   \n",
      "10667       7815                                                NaN  NaN  ...   \n",
      "10668       7816  04-Apr-2023 - BBA - Wipernot working properly ...  RTI  ...   \n",
      "10669       7817                                                NaN  NaN  ...   \n",
      "\n",
      "      FIRST DETECTION LAST DETECTION LAST UPDATE OF DETECTION PILOTE  \\\n",
      "0                 NaN            NaN                             NaN   \n",
      "1                 NaN            NaN                             NaN   \n",
      "2                 NaN            NaN                             NaN   \n",
      "3                 NaN            NaN                             NaN   \n",
      "4                 NaN            NaN                             NaN   \n",
      "...               ...            ...                             ...   \n",
      "10665      04/04/2023     01/05/2023                      04/04/2023   \n",
      "10666      02/04/2023     04/04/2023                      04/04/2023   \n",
      "10667             NaN            NaN                             NaN   \n",
      "10668      04/04/2023     12/04/2023                      29/09/2023   \n",
      "10669             NaN            NaN                             NaN   \n",
      "\n",
      "      LAST UPDATE OF INCIDENT PILOTE                           KEYWORDS  \\\n",
      "0                                NaN                                NaN   \n",
      "1                                NaN                                NaN   \n",
      "2                                NaN                                NaN   \n",
      "3                                NaN                                NaN   \n",
      "4                                NaN                                NaN   \n",
      "...                              ...                                ...   \n",
      "10665                            NaN  acceleration reverse direction EN   \n",
      "10666                            NaN                    wheel locked EN   \n",
      "10667                            NaN                                NaN   \n",
      "10668                            NaN  not switching working switches EN   \n",
      "10669                            NaN                                NaN   \n",
      "\n",
      "                                         DETECTION QUERY source creation date  \\\n",
      "0                                                    NaN                  NaT   \n",
      "1                                                    NaN                  NaT   \n",
      "2                                                    NaN                  NaT   \n",
      "3                                                    NaN                  NaT   \n",
      "4                                                    NaN                  NaT   \n",
      "...                                                  ...                  ...   \n",
      "10665  RTI - EICps Déficience dans la motricité / Tra...           2023-04-01   \n",
      "10666  RTI - EICps Déficience dans la motricité / Tra...                  NaT   \n",
      "10667                                                NaN                  NaT   \n",
      "10668  RTI - EICps Mauvaise signalisation / Bad signa...           2023-03-28   \n",
      "10669                                                NaN           2023-04-11   \n",
      "\n",
      "       year  month day  \n",
      "0         0      0   0  \n",
      "1         0      0   0  \n",
      "2         0      0   0  \n",
      "3         0      0   0  \n",
      "4         0      0   0  \n",
      "...     ...    ...  ..  \n",
      "10665  2023      4   1  \n",
      "10666     0      0   0  \n",
      "10667     0      0   0  \n",
      "10668  2023      3  28  \n",
      "10669  2023      4  11  \n",
      "\n",
      "[10670 rows x 141 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "# Fill NaN values in 'year', 'month', and 'day' columns with zeros\n",
    "df['year'] = df['year'].fillna(0).astype(int)\n",
    "df['month'] = df['month'].fillna(0).astype(int)\n",
    "df['day'] = df['day'].fillna(0).astype(int)\n",
    "\n",
    "# Print the DataFrame to verify changes\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4f8d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    month   season\n",
      "0       1  Monsoon\n",
      "1       2   Summer\n",
      "2       3   Summer\n",
      "3       4   Summer\n",
      "4       5   Summer\n",
      "5       6   Summer\n",
      "6       7     Wind\n",
      "7       8     Wind\n",
      "8       9     Wind\n",
      "9      10  Monsoon\n",
      "10     11  Monsoon\n",
      "11     12  Monsoon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all months (1 to 12)\n",
    "df = pd.DataFrame({'month': range(1, 13)})\n",
    "\n",
    "# Define a function to map numerical month values to seasons\n",
    "def map_season(month):\n",
    "    if month in range(2, 7):\n",
    "        return 'Summer'\n",
    "    elif month in [10, 11, 12, 1]:\n",
    "        return 'Monsoon'\n",
    "    else:\n",
    "        return 'Wind'\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df['month'].apply(map_season)\n",
    "\n",
    "# Print the DataFrame with 'season' column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2493cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['INCIDENT N°', 'PROTECTIONS', 'ASSIGNED TO TOPICS',\n",
      "       'TOPICS OF THE OPERATION', 'STATE', 'TRANSFERED FROM', 'TRANSFERRED TO',\n",
      "       'Dummy VIN', 'GI COMMENT', 'TAGS',\n",
      "       ...\n",
      "       'BLOCKING FOR TEST ?', 'ALGO', 'DETECTION PILOT', 'INCIDENT PILOT',\n",
      "       'FIRST DETECTION', 'LAST DETECTION', 'LAST UPDATE OF DETECTION PILOTE',\n",
      "       'LAST UPDATE OF INCIDENT PILOTE', 'KEYWORDS', 'DETECTION QUERY'],\n",
      "      dtype='object', length=137)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a475c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['day', 'month', 'year', 'INCIDENT N°', 'PROTECTIONS',\n",
      "       'ASSIGNED TO TOPICS', 'TOPICS OF THE OPERATION', 'STATE',\n",
      "       'TRANSFERED FROM', 'TRANSFERRED TO',\n",
      "       ...\n",
      "       'FIRST DETECTION', 'LAST DETECTION', 'LAST UPDATE OF DETECTION PILOTE',\n",
      "       'LAST UPDATE OF INCIDENT PILOTE', 'KEYWORDS', 'DETECTION QUERY',\n",
      "       'source creation date', 'year', 'month', 'day'],\n",
      "      dtype='object', length=144)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from Excel file with header starting from the second row\n",
    "df = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "df['PIMOF'] = df['PIMOF'].replace({'yes': 1, 'no': 0})\n",
    "# Assuming your date column is named 'source creation date'\n",
    "# Convert the 'source creation date' column to datetime format\n",
    "df['source creation date'] = pd.to_datetime(df['SOURCE CREATION DATE'], format='%d/%m/%Y')\n",
    "\n",
    "# Extract year, month, and day into separate columns\n",
    "df['year'] = df['source creation date'].dt.year\n",
    "df['month'] = df['source creation date'].dt.month\n",
    "df['day'] = df['source creation date'].dt.day\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df = pd.concat([df[['day', 'month', 'year']], df], axis=1)\n",
    "\n",
    "# Print the DataFrame with separated columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dab0b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10328\\4266256194.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['PIMOF'] = label_encoder.fit_transform(X['PIMOF'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '21/01/2024'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10328\\4266256194.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Scaling the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m    840\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    842\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '21/01/2024'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have already loaded the DataFrame and added the 'year', 'month', 'day', and 'season' columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical variables to numeric values\n",
    "X['PIMOF'] = label_encoder.fit_transform(X['PIMOF'])\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selecting columns for clustering\n",
    "X = df[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']]\n",
    "\n",
    "# Ensure 'day', 'month', and 'year' columns are numeric\n",
    "X['day'] = pd.to_numeric(X['day'], errors='coerce')\n",
    "X['month'] = pd.to_numeric(X['month'], errors='coerce')\n",
    "X['year'] = pd.to_numeric(X['year'], errors='coerce')\n",
    "\n",
    "# Handling missing values\n",
    "X.fillna(0, inplace=True)  # Replace missing values with 0 or any other appropriate value\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Selecting columns for clustering\n",
    "X = df[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'SOURCE CREATION DATE']]\n",
    "\n",
    "# Handling missing values\n",
    "X.fillna(0, inplace=True)  # Fill missing values with 0, assuming missing values indicate absence of information\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize KMeans clustering with a chosen number of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit KMeans clustering to the scaled data\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Assign cluster labels to the original DataFrame\n",
    "df['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# Print the DataFrame with cluster labels\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe7b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INCIDENT N° PROTECTIONS ASSIGNED TO TOPICS TOPICS OF THE OPERATION STATE  \\\n",
      "0    212472461         NaN                NaN                     NaN     -   \n",
      "1    212472456         NaN                NaN                     NaN     -   \n",
      "2    212467991         NaN                NaN                     NaN     -   \n",
      "3    212468029         NaN                NaN                     NaN     -   \n",
      "4    212465847         NaN                NaN                     NaN     -   \n",
      "\n",
      "   TRANSFERED FROM  TRANSFERRED TO  Dummy VIN GI COMMENT TAGS  ...  \\\n",
      "0              NaN             NaN          1        NaN  NaN  ...   \n",
      "1              NaN             NaN          2        NaN  NaN  ...   \n",
      "2              NaN             NaN          3        NaN  NaN  ...   \n",
      "3              NaN             NaN          4        NaN  NaN  ...   \n",
      "4              NaN             NaN          5        NaN  NaN  ...   \n",
      "\n",
      "  BLOCKING FOR TEST ? ALGO DETECTION PILOT INCIDENT PILOT FIRST DETECTION  \\\n",
      "0                 NaN  NaN             NaN            NaN             NaN   \n",
      "1                 NaN  NaN             NaN            NaN             NaN   \n",
      "2                 NaN  NaN             NaN            NaN             NaN   \n",
      "3                 NaN  NaN             NaN            NaN             NaN   \n",
      "4                 NaN  NaN             NaN            NaN             NaN   \n",
      "\n",
      "  LAST DETECTION LAST UPDATE OF DETECTION PILOTE  \\\n",
      "0            NaN                             NaN   \n",
      "1            NaN                             NaN   \n",
      "2            NaN                             NaN   \n",
      "3            NaN                             NaN   \n",
      "4            NaN                             NaN   \n",
      "\n",
      "   LAST UPDATE OF INCIDENT PILOTE  KEYWORDS DETECTION QUERY  \n",
      "0                             NaN       NaN             NaN  \n",
      "1                             NaN       NaN             NaN  \n",
      "2                             NaN       NaN             NaN  \n",
      "3                             NaN       NaN             NaN  \n",
      "4                             NaN       NaN             NaN  \n",
      "\n",
      "[5 rows x 137 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f344f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    NaN\n",
      "1    NaN\n",
      "2    NaN\n",
      "3    NaN\n",
      "4    NaN\n",
      "Name: SOURCE CREATION DATE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and 'column_name' is the name of the column you want to inspect\n",
    "column_head = df['SOURCE CREATION DATE'].head()\n",
    "print(column_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5043f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day  month  year\n",
      "0  NaN    NaN   NaN\n",
      "1  NaN    NaN   NaN\n",
      "2  NaN    NaN   NaN\n",
      "3  NaN    NaN   NaN\n",
      "4  NaN    NaN   NaN\n",
      "        day  month    year\n",
      "10665   4.0    1.0  2023.0\n",
      "10666   NaN    NaN     NaN\n",
      "10667   NaN    NaN     NaN\n",
      "10668  28.0    3.0  2023.0\n",
      "10669   4.0   11.0  2023.0\n"
     ]
    }
   ],
   "source": [
    "# Convert 'SOURCE CREATION DATE' column to datetime format with specified format\n",
    "df['SOURCE CREATION DATE'] = pd.to_datetime(df['SOURCE CREATION DATE'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Extract day, month, and year into separate columns\n",
    "df['day'] = df['SOURCE CREATION DATE'].dt.day\n",
    "df['month'] = df['SOURCE CREATION DATE'].dt.month\n",
    "df['year'] = df['SOURCE CREATION DATE'].dt.year\n",
    "\n",
    "# Display the head of the new columns\n",
    "print(df[['day', 'month', 'year']].head())\n",
    "print(df[['day', 'month', 'year']].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e47f455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INCIDENT N°  PIMOF  MIS  MILEAGE (KM)   day  month    year  cluster\n",
      "72    212429057      0   12         22636  21.0    1.0  2024.0        2\n",
      "75    212429018      0   16          4923  18.0    1.0  2024.0        2\n",
      "80    212420384      0   16          9124  19.0    1.0  2024.0        2\n",
      "81    212420807      0   12         22191  19.0    1.0  2024.0        2\n",
      "82    212420285      0    7         21356  18.0    1.0  2024.0        2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with the specified columns\n",
    "# Drop rows with NaN values as KMeans cannot handle them\n",
    "df = df.dropna(subset=['day', 'month', 'year', 'INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)'])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode the 'PIMOF' column\n",
    "label_encoder = LabelEncoder()\n",
    "df['PIMOF'] = label_encoder.fit_transform(df['PIMOF'])\n",
    "\n",
    "# Now you can proceed with scaling and clustering\n",
    "\n",
    "# Selecting the columns for clustering\n",
    "X = df[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize KMeans clustering with a chosen number of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit KMeans clustering model to the data\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Display the DataFrame with cluster labels\n",
    "print(df[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year', 'cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26ea17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for 2 clusters: 0.2304901607683982\n",
      "Silhouette score for 3 clusters: 0.269564261551279\n",
      "Silhouette score for 4 clusters: 0.3027371250700183\n",
      "Silhouette score for 5 clusters: 0.2543481782077171\n",
      "Silhouette score for 6 clusters: 0.2546911775492127\n",
      "Silhouette score for 7 clusters: 0.26696702471488565\n",
      "Silhouette score for 8 clusters: 0.2693973400465679\n",
      "Silhouette score for 9 clusters: 0.27912564244759264\n",
      "Silhouette score for 10 clusters: 0.2875960134021934\n",
      "Silhouette score for 11 clusters: 0.29594068925571415\n",
      "Silhouette score for 12 clusters: 0.28903570840256\n",
      "Silhouette score for 13 clusters: 0.28566269130603666\n",
      "Silhouette score for 14 clusters: 0.2707407457702138\n",
      "Silhouette score for 15 clusters: 0.2672693626279463\n",
      "Silhouette score for 16 clusters: 0.27272360093144893\n",
      "Silhouette score for 17 clusters: 0.2693735169048412\n",
      "Silhouette score for 18 clusters: 0.2741644330132871\n",
      "Silhouette score for 19 clusters: 0.27410486514310456\n",
      "Silhouette score for 20 clusters: 0.27034633566521876\n",
      "Optimal number of clusters: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Initialize an empty list to store silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Specify the range of clusters to try\n",
    "max_clusters = 20\n",
    "for i in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Find the index of the maximum silhouette score\n",
    "optimal_index = silhouette_scores.index(max(silhouette_scores))\n",
    "\n",
    "# Print silhouette scores for all clusters\n",
    "for i, score in enumerate(silhouette_scores, start=2):\n",
    "    print(f\"Silhouette score for {i} clusters: {score}\")\n",
    "\n",
    "# Print the optimal number of clusters\n",
    "optimal_clusters = optimal_index + 2  # Adding 2 because range starts from 2 clusters\n",
    "print(\"Optimal number of clusters:\", optimal_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a390cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10328\\1049468376.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.dropna(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.3027371250700183\n",
      "Cluster Labels:\n",
      "[1 1 1 ... 3 0 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select columns for clustering\n",
    "selected_columns = ['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']\n",
    "\n",
    "# Subset the dataframe with selected columns\n",
    "X = df[selected_columns]\n",
    "\n",
    "# Drop rows with missing values\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Fit the model to the scaled data\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Display cluster labels\n",
    "print(\"Cluster Labels:\")\n",
    "print(cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "706f7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10328\\2841708486.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.fillna(0, inplace=True)  # Filling missing values with 0 for simplicity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Silhouette Score: 0.06624978727494457\n",
      "Confusion Matrix:\n",
      "[[666   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   0]\n",
      " [  0   0   7 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   6   0   0]\n",
      " [  0   0   0 ...   0   5   0]\n",
      " [  0   0   0 ...   0   0   9]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "\n",
    "# Assuming df is your DataFrame containing the data\n",
    "# Selecting columns of interest\n",
    "columns_of_interest = ['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']\n",
    "X = df[columns_of_interest]\n",
    "\n",
    "# Preprocessing the data\n",
    "# Handling missing values (you can use other methods depending on your data)\n",
    "X.fillna(0, inplace=True)  # Filling missing values with 0 for simplicity\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applying DBSCAN clustering\n",
    "eps = 0.5  # Adjust as needed\n",
    "min_samples = 5  # Adjust as needed\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "cluster_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "print(\"DBSCAN Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Display confusion matrix\n",
    "# Since DBSCAN does not require specifying the number of clusters, we'll use cluster labels directly\n",
    "conf_matrix = confusion_matrix(cluster_labels, cluster_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f31a2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming X contains your data\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(X)\n",
    "\n",
    "# Find the number of components explaining a desired percentage of variance\n",
    "desired_variance = 0.95  # Adjust as needed\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(n_components)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30096787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Silhouette Score: 0.23049016076840076\n",
      "Confusion Matrix:\n",
      "[[3995 4518]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "\n",
    "# Assuming X contains your data with columns 'INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', and 'year'\n",
    "\n",
    "# Selecting columns for PCA\n",
    "X_pca = X[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']]\n",
    "\n",
    "# Handling missing values if any\n",
    "X_pca = X_pca.dropna()\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Determine the number of components to explain 95% of the variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# Reduce dimensionality to the determined number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca_transformed = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Perform clustering on the PCA-transformed data\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_pca_transformed)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(X_pca_transformed, cluster_labels)\n",
    "print(\"PCA Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Print confusion matrix\n",
    "y_true = [0] * len(X_pca_transformed)  # Assuming we don't have ground truth labels\n",
    "conf_matrix = confusion_matrix(y_true, cluster_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1a0a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\python\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE Silhouette Score: 0.38877115\n",
      "Confusion Matrix:\n",
      "[[2317 1171 2554 2471]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]\n",
      " [   0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "\n",
    "# Assuming X contains your data with columns 'INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', and 'year'\n",
    "\n",
    "# Selecting columns for t-SNE\n",
    "X_tsne = X[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']]\n",
    "\n",
    "# Handling missing values if any\n",
    "X_tsne = X_tsne.dropna()\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_tsne)\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(random_state=42)\n",
    "\n",
    "# Fit t-SNE\n",
    "X_tsne_transformed = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Perform clustering on the t-SNE-transformed data\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_tsne_transformed)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(X_tsne_transformed, cluster_labels)\n",
    "print(\"t-SNE Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Print confusion matrix\n",
    "y_true = [0] * len(X_tsne_transformed)  # Assuming we don't have ground truth labels\n",
    "conf_matrix = confusion_matrix(y_true, cluster_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8128e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\python\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: packaging in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34a744af",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10328\\3229399413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[1;31m# line: 456\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[1;31m# line: 365\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[1;31m# line: 418\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    417\u001b[0m \"\"\"\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_autograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnested_structure_coder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_saved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaveable_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_checkpoint_reader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Assuming X contains your data with columns 'INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', and 'year'\n",
    "\n",
    "# Selecting columns for autoencoder\n",
    "X_autoencoder = X[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']]\n",
    "\n",
    "# Handling missing values if any\n",
    "X_autoencoder = X_autoencoder.dropna()\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_autoencoder)\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "encoding_dim = 3  # Adjust this number as needed\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the autoencoder\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=10, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Get the encoded representations of the data\n",
    "encoder = Model(input_layer, encoded)\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# Perform clustering on the encoded data\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_encoded)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(X_encoded, cluster_labels)\n",
    "print(\"Autoencoder Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Print confusion matrix\n",
    "y_true = [0] * len(X_encoded)  # Assuming we don't have ground truth labels\n",
    "conf_matrix = confusion_matrix(y_true, cluster_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5329e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['INCIDENT N°', 'PROTECTIONS', 'ASSIGNED TO TOPICS',\n",
      "       'TOPICS OF THE OPERATION', 'STATE', 'TRANSFERED FROM', 'TRANSFERRED TO',\n",
      "       'Dummy VIN', 'GI COMMENT', 'TAGS',\n",
      "       ...\n",
      "       'INCIDENT PILOT', 'FIRST DETECTION', 'LAST DETECTION',\n",
      "       'LAST UPDATE OF DETECTION PILOTE', 'LAST UPDATE OF INCIDENT PILOTE',\n",
      "       'KEYWORDS', 'DETECTION QUERY', 'day', 'month', 'year'],\n",
      "      dtype='object', length=140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx',header=1)\n",
    "\n",
    "# Convert 'SOURCE CREATION DATE' to datetime format and extract day, month, and year\n",
    "data['SOURCE CREATION DATE'] = pd.to_datetime(data['SOURCE CREATION DATE'], format='%d/%m/%Y')\n",
    "data['day'] = data['SOURCE CREATION DATE'].dt.day\n",
    "data['month'] = data['SOURCE CREATION DATE'].dt.month\n",
    "data['year'] = data['SOURCE CREATION DATE'].dt.year\n",
    "\n",
    "# Now 'day', 'month', and 'year' columns are added to your DataFrame\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b5e9331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day  month  year\n",
      "72   21      1  2024\n",
      "75   18      1  2024\n",
      "80   19      1  2024\n",
      "81   19      1  2024\n",
      "82   18      1  2024\n",
      "       day  month  year\n",
      "10660    4      1  2023\n",
      "10664    4      1  2023\n",
      "10665    4      1  2023\n",
      "10668   28      3  2023\n",
      "10669    4     11  2023\n"
     ]
    }
   ],
   "source": [
    "# Convert 'SOURCE CREATION DATE' column to datetime format with specified format\n",
    "df['SOURCE CREATION DATE'] = pd.to_datetime(df['SOURCE CREATION DATE'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Extract day, month, and year into separate columns\n",
    "df['day'] = df['SOURCE CREATION DATE'].dt.day\n",
    "df['month'] = df['SOURCE CREATION DATE'].dt.month\n",
    "df['year'] = df['SOURCE CREATION DATE'].dt.year\n",
    "\n",
    "# Display the head of the new columns\n",
    "print(df[['day', 'month', 'year']].head())\n",
    "print(df[['day', 'month', 'year']].tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94000f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INCIDENT N°  PIMOF  MIS  MILEAGE (KM)  day  month  year\n",
      "72    212429057      0   12         22636   21      1  2024\n",
      "75    212429018      0   16          4923   18      1  2024\n",
      "80    212420384      0   16          9124   19      1  2024\n",
      "81    212420807      0   12         22191   19      1  2024\n",
      "82    212420285      0    7         21356   18      1  2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with the existing columns\n",
    "# Assuming you have already created 'day', 'month', and 'year' columns from 'SOURCE CREATION DATE'\n",
    "\n",
    "# Concatenate the new columns with the existing DataFrame\n",
    "df = pd.concat([df[['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)']], df[['day', 'month', 'year']]], axis=1)\n",
    "\n",
    "# Now df contains all the original columns along with the new 'day', 'month', and 'year' columns\n",
    "print(df.head())  # Check the DataFrame to verify the concatenation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f080c787",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10328\\4129632027.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[1;31m# line: 456\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[1;31m# line: 365\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[1;31m# line: 418\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_autograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnested_structure_coder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_saved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaveable_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_checkpoint_reader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = ['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']\n",
    "X = data[selected_columns]\n",
    "\n",
    "# Handle missing values if any\n",
    "# For example, you can fill missing numerical values with mean and categorical values with mode\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the architecture of the VAE model\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(X_train_scaled.shape[1]),\n",
    "        ])\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "# Define the loss function\n",
    "def vae_loss(x, x_recon, mean, logvar):\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(x - x_recon))\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=1)\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Instantiate the VAE model\n",
    "latent_dim = 2\n",
    "vae = VAE(latent_dim)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "\n",
    "# Train the model\n",
    "history = vae.fit(X_train_scaled, X_train_scaled, epochs=100, batch_size=32, validation_data=(X_test_scaled, X_test_scaled))\n",
    "\n",
    "# Reconstruction error\n",
    "X_pred = vae.predict(X_test_scaled)\n",
    "reconstruction_error = np.mean(np.square(X_test_scaled - X_pred))\n",
    "print(\"Reconstruction Error:\", reconstruction_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a990d00",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10328\\1901272057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[1;31m# line: 456\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[1;31m# line: 365\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[1;31m# line: 418\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_autograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnested_structure_coder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_saved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaveable_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_checkpoint_reader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns\n",
    "selected_columns = ['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)', 'day', 'month', 'year']\n",
    "X = data[selected_columns]\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define GAN architecture\n",
    "generator = Sequential([\n",
    "    Dense(128, input_shape=(latent_dim,), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(X_scaled.shape[1], activation='linear')  # Output layer\n",
    "])\n",
    "\n",
    "discriminator = Sequential([\n",
    "    Dense(64, input_shape=(X_scaled.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Freeze discriminator layers during generator training\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Combined model\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "latent_dim = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "    fake_data = generator.predict(noise)\n",
    "    real_data = X_scaled[np.random.randint(0, X_scaled.shape[0], size=batch_size)]\n",
    "\n",
    "    X_combined = np.concatenate([real_data, fake_data])\n",
    "    y_combined = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "\n",
    "    discriminator_loss = discriminator.train_on_batch(X_combined, y_combined)\n",
    "\n",
    "    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "    y_gen = np.ones((batch_size, 1))\n",
    "\n",
    "    generator_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}')\n",
    "\n",
    "# Evaluate the GAN\n",
    "# You can generate samples using the generator and evaluate them based on your specific task\n",
    "\n",
    "# Note: GANs for tabular data are not commonly used and may not perform well. Consider other approaches for tabular data generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384dad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "              INCIDENT N°       MIS  MILEAGE (KM)\n",
      "INCIDENT N°      1.000000  0.370103      0.177626\n",
      "MIS              0.370103  1.000000      0.506810\n",
      "MILEAGE (KM)     0.177626  0.506810      1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['INCIDENT N°', 'PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = data[columns_of_interest].corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98d874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.50235928 0.33329224 0.16434848]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in any of the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf2366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.50235928 0.33329224 0.16434848]\n",
      "Reconstruction Error: 9.856833730090884e-31\n",
      "PCA Components:\n",
      "      PIMOF       MIS  MILEAGE (KM)\n",
      "0  0.022999  0.706782      0.707057\n",
      "1  0.999592 -0.028224     -0.004302\n",
      "2 -0.016915 -0.706868      0.707143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in any of the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "reconstruction_error = ((X_scaled - X_reconstructed) ** 2).mean()\n",
    "\n",
    "print(\"Reconstruction Error:\", reconstruction_error)\n",
    "\n",
    "# Get the PCA components\n",
    "components = pd.DataFrame(pca.components_, columns=columns_of_interest)\n",
    "\n",
    "print(\"PCA Components:\")\n",
    "print(components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ebf0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Frequent Itemsets:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in any of the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Define a function to generate frequent itemsets using the Apriori algorithm\n",
    "def generate_frequent_itemsets(data, min_support):\n",
    "    transactions = data.apply(lambda row: set(row.dropna()), axis=1)\n",
    "    num_transactions = len(transactions)\n",
    "    frequent_itemsets = {}\n",
    "    all_items = set(data.columns)\n",
    "    \n",
    "    # Generate frequent itemsets of length 1\n",
    "    candidates = [(item,) for item in all_items]\n",
    "    frequent_itemsets[1] = {}\n",
    "    for item in candidates:\n",
    "        support = sum(1 for transaction in transactions if set(item).issubset(transaction)) / num_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets[1][item] = support\n",
    "    \n",
    "    # Generate frequent itemsets of length > 1\n",
    "    k = 2\n",
    "    while frequent_itemsets[k - 1]:\n",
    "        frequent_itemsets[k] = {}\n",
    "        candidates = set()\n",
    "        for itemset in frequent_itemsets[k - 1]:\n",
    "            for item in all_items.difference(set(itemset)):\n",
    "                candidate = tuple(sorted(list(itemset) + [item]))\n",
    "                candidates.add(candidate)\n",
    "        for candidate in candidates:\n",
    "            support = sum(1 for transaction in transactions if set(candidate).issubset(transaction)) / num_transactions\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets[k][candidate] = support\n",
    "        k += 1\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "min_support = 0.1\n",
    "frequent_itemsets = generate_frequent_itemsets(data[columns_of_interest], min_support)\n",
    "\n",
    "# Print scores and matrix\n",
    "print(\"Scores for Frequent Itemsets:\")\n",
    "for k, itemsets in frequent_itemsets.items():\n",
    "    for itemset, support in itemsets.items():\n",
    "        print(f\"Support for {itemset}: {support}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27e233ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM Scores:\n",
      "-7.650043378734142\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Fit Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(data[columns_of_interest])\n",
    "\n",
    "# Print the GMM scores\n",
    "print(\"GMM Scores:\")\n",
    "print(gmm.score(data[columns_of_interest]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769e6bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New samples generated by KDE:\n",
      "[[-8.20026827e-01  9.39581941e+00  1.89239661e+04]\n",
      " [ 1.75455063e-01  1.11319580e+01  3.34889031e+03]\n",
      " [ 1.36436351e+00  6.53375946e+00  9.01806476e+03]\n",
      " [ 1.45130417e+00  1.90343313e+00  3.21153629e+02]\n",
      " [ 1.34734074e+00  6.31406346e-01  4.72938853e+03]\n",
      " [ 2.49852035e-01  9.91905342e+00  8.32857588e+03]\n",
      " [ 1.31818053e+00  1.38594635e+01  2.70659446e+03]\n",
      " [ 8.23520212e-01  6.39052763e+00  1.06427542e+04]\n",
      " [ 8.07509025e-01  9.21846334e+00  2.42513720e+04]\n",
      " [ 5.84760021e-02  1.24981451e+01  1.07730919e+04]\n",
      " [-6.52156366e-01  1.21525662e+01  1.00178476e+04]\n",
      " [-6.73476623e-01  2.50647238e+00  2.85520250e+03]\n",
      " [ 1.42503096e+00  6.82704440e+00  6.21474748e+03]\n",
      " [-2.57177315e-01  6.52230624e+00  1.19360020e+04]\n",
      " [ 4.63414547e-02  7.18200854e+00  2.19541883e+03]\n",
      " [ 1.28427718e+00  6.98187712e+00  1.12506214e+04]\n",
      " [-4.24091423e-01  6.13454301e+00  4.44028392e+03]\n",
      " [ 1.31324122e+00  4.08086004e+00  1.53796427e+04]\n",
      " [-7.63643779e-01 -6.05005546e-02  3.25348007e+01]\n",
      " [ 1.53649204e+00  3.84750190e+00  9.99974993e+03]\n",
      " [ 1.92848544e+00  1.07553487e+01  1.67488576e+04]\n",
      " [ 5.70781496e-01  1.26406283e+01  8.87434790e+03]\n",
      " [-5.72528523e-01  1.01057473e+01  8.92208604e+03]\n",
      " [ 9.69362579e-01  1.15057580e+01  4.46887088e+03]\n",
      " [-7.27466598e-01  5.92157972e+00  1.22527863e+04]\n",
      " [-5.92212311e-01  9.98325440e-01  6.56193949e+02]\n",
      " [ 1.51702947e+00  1.18441945e+01  1.75906872e+04]\n",
      " [-6.47047153e-01  9.24949858e+00  2.07165841e+04]\n",
      " [-3.14949488e-01  3.73729246e+00  3.23197709e+03]\n",
      " [ 8.20047241e-01  3.14274896e+00  2.90247529e+03]\n",
      " [ 3.94137911e-01  5.78329794e+00  7.89247529e+03]\n",
      " [-7.77999742e-01  1.29451703e+01  9.73089914e+03]\n",
      " [ 2.17578930e+00  1.45956626e+00  4.10292040e+03]\n",
      " [-1.24447591e-01  1.00515116e+01  7.38500831e+03]\n",
      " [ 1.60037307e+00  5.76802222e+00  6.10301708e+03]\n",
      " [-3.85167977e-01  1.09437309e+01  3.58062514e+04]\n",
      " [-1.03304669e+00  1.06120109e+01  2.12764890e+04]\n",
      " [-7.01666306e-01  9.83699309e+00  1.16604352e+04]\n",
      " [ 3.06622142e-01  1.29741369e+01  1.00724690e+04]\n",
      " [-1.33603605e-01  1.05031661e+01  1.08205831e+04]\n",
      " [ 5.01401636e-01 -1.74287394e-01  1.23040936e+03]\n",
      " [ 2.15873692e-01  1.40669105e+01  2.98842072e+03]\n",
      " [-1.63701788e+00  1.04157972e+01  1.50370874e+04]\n",
      " [ 5.05235959e-01  2.68194009e+00  2.90158861e+03]\n",
      " [-6.71834506e-01  8.81977274e+00  1.68496718e+04]\n",
      " [-3.09449125e-01  6.15714080e+00  5.13678078e+03]\n",
      " [-1.78430999e+00  2.06476977e+00  5.97901461e+03]\n",
      " [-8.51065632e-01  1.03751813e+01  1.20115281e+04]\n",
      " [ 4.87652211e-01  8.96196358e+00  1.00896088e+04]\n",
      " [-1.58143782e-01  9.23993689e+00  3.53894257e+03]\n",
      " [-4.74757500e-02  9.74401600e+00  2.08204671e+04]\n",
      " [-3.61442592e-01  8.90052436e+00  1.30452907e+04]\n",
      " [ 1.58377180e+00  1.15521200e+01  1.12313428e+04]\n",
      " [-1.57237175e-02  1.13310559e+01  7.00841644e+03]\n",
      " [ 8.41045492e-01  1.36601808e+01  3.94314563e+03]\n",
      " [ 8.65023156e-01 -1.66719204e+00  3.62808653e+02]\n",
      " [-3.23411542e-01  9.97455704e+00  4.06767623e+03]\n",
      " [ 5.00638949e-01  1.30856871e+01  1.72810557e+04]\n",
      " [ 1.19385083e+00  2.43054080e+00  1.23164699e+03]\n",
      " [ 1.26167492e+00  1.00837563e+01  5.06165638e+03]\n",
      " [-1.19214499e+00  7.16869998e-01  2.68376752e+01]\n",
      " [ 1.70559784e-01  2.72581936e+00  1.92253876e+03]\n",
      " [-1.46103224e+00  1.32776051e+01  9.42211751e+03]\n",
      " [ 2.73964491e-01  1.25516832e+00  1.95994784e+03]\n",
      " [-1.86615247e-02  1.23433347e+01  1.26652156e+04]\n",
      " [ 2.20554127e-01  1.29088412e+01  9.16698665e+03]\n",
      " [-2.06416341e-01  1.23685108e+01  1.90741456e+04]\n",
      " [ 1.49907075e+00  5.44002381e+00  1.84884924e+04]\n",
      " [ 1.04785191e+00  1.13529295e+01  1.32029901e+04]\n",
      " [-6.50965526e-01  8.48442620e+00  8.52352013e+03]\n",
      " [ 2.57998402e-01  6.99397387e+00  3.34364822e+03]\n",
      " [ 2.66597843e-01  9.50501010e+00  8.47693832e+03]\n",
      " [ 1.15308824e+00  9.63913951e+00  9.94623772e+03]\n",
      " [ 6.12188054e-01  8.16663079e+00  8.84571908e+03]\n",
      " [-5.06783641e-01  1.18684667e+01  1.90538003e+04]\n",
      " [-6.13378349e-01  4.42357304e+00  5.64094116e+03]\n",
      " [ 1.05957592e+00  1.16092238e+01  7.53829805e+03]\n",
      " [-1.49682571e+00  2.93149107e+00  1.36875822e+04]\n",
      " [ 2.23645385e+00  7.64335477e+00  3.61444443e+03]\n",
      " [-1.01951532e+00  1.87378373e+00  1.40238067e+03]\n",
      " [ 2.47228091e+00  4.80287547e+00  4.19772349e+03]\n",
      " [-1.85267246e-01  1.34043834e+01  4.57692125e+04]\n",
      " [ 1.97102800e-01  6.76141075e+00  9.30278701e+03]\n",
      " [ 7.13477621e-02  9.75539964e+00  1.84021936e+04]\n",
      " [ 4.18349675e-01  8.25469374e+00  2.12586990e+04]\n",
      " [-6.48790534e-01  6.74472716e+00  1.03664723e+04]\n",
      " [ 1.53015844e+00  3.70602555e+00  2.91865108e+03]\n",
      " [ 2.24196589e-01  7.02714599e+00  1.59564784e+04]\n",
      " [-1.17847556e+00  1.04593682e+01  6.41221519e+03]\n",
      " [-2.06363028e-01  6.11183661e+00  7.76217689e+03]\n",
      " [ 2.04183066e+00  8.09870114e+00  2.01894545e+04]\n",
      " [ 1.05031947e+00  7.24536161e+00  1.75910265e+04]\n",
      " [ 2.07906643e+00  4.33477855e+00  5.77599694e+02]\n",
      " [ 4.02248095e-02  1.37783785e+01  7.62078965e+03]\n",
      " [-4.44516188e-01  8.95354809e+00  1.83862607e+04]\n",
      " [-1.44117854e+00  5.84459374e+00  2.52761641e+03]\n",
      " [ 2.82378788e-01  2.54051357e+00  2.11729145e+03]\n",
      " [ 6.13330471e-01  9.83638004e-01  3.14990481e+03]\n",
      " [-9.34369462e-01  1.16684592e+01  2.68760455e+04]\n",
      " [-1.54010590e-01  1.45790802e+00  2.29375871e+03]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in any of the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Extract numeric values from selected columns\n",
    "X = data[columns_of_interest].values\n",
    "\n",
    "# Fit the KDE model to the data\n",
    "kde = KernelDensity(kernel='gaussian')\n",
    "kde.fit(X)\n",
    "\n",
    "# Sample from the KDE model to obtain new features\n",
    "new_samples = kde.sample(100)  # Adjust the number of samples as needed\n",
    "\n",
    "print(\"New samples generated by KDE:\")\n",
    "print(new_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ad4488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "[  19.02836001 -402.84881176 -268.82811488 ... -181.43678002 -951.73076845\n",
      "  -90.80589951]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[columns_of_interest] = imputer.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Fit One-Class SVM model\n",
    "ocsvm = OneClassSVM(kernel='rbf')\n",
    "ocsvm.fit(X)\n",
    "# Obtain scores for each sample\n",
    "scores = ocsvm.decision_function(X)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Scores:\")\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ace85f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[ 1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Obtain binary predictions (1 for inliers, -1 for outliers)\n",
    "predictions = ocsvm.predict(X)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88e02384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels:\n",
      "[0 0 0 ... 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Drop rows with missing values in any of the selected columns\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n",
    "# Convert non-numeric columns to numeric using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in columns_of_interest:\n",
    "    if data[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Preprocessing: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Fit Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)  # You can adjust the number of components as needed\n",
    "gmm.fit(X)\n",
    "\n",
    "# Get cluster assignments for each sample\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(\"Cluster Labels:\")\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f65b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.4132425300037363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Compute the silhouette score\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "848f28cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Data Shape: (10670, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3432\\3345453342.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[col] = label_encoders[col].fit_transform(data_filtered[col])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class VariationalAutoencoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.pca = PCA(n_components=latent_dim)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # Preprocess the data\n",
    "        X_train_imputed = self.imputer.fit_transform(X_train)\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_imputed)\n",
    "        \n",
    "        # Fit PCA\n",
    "        self.pca.fit(X_train_scaled)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        # Preprocess the data\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_imputed)\n",
    "        \n",
    "        # Encode the data\n",
    "        X_encoded = self.pca.transform(X_scaled)\n",
    "        \n",
    "        return X_encoded\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Filter data\n",
    "data_filtered = data[columns_of_interest]\n",
    "\n",
    "# Handle non-numeric data\n",
    "label_encoders = {}\n",
    "for col in columns_of_interest:\n",
    "    if data_filtered[col].dtype == 'object':\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        data_filtered[col] = label_encoders[col].fit_transform(data_filtered[col])\n",
    "\n",
    "# Initialize and fit VAE model\n",
    "vae = VariationalAutoencoder(latent_dim=2)\n",
    "vae.fit(data_filtered)\n",
    "\n",
    "# Transform data\n",
    "data_encoded = vae.transform(data_filtered)\n",
    "\n",
    "print(\"Encoded Data Shape:\", data_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85678ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Algorithms:\n",
      "K-means Score: 0.42966827689911663\n",
      "Hierarchical Clustering Score: 0.3132061510996466\n",
      "DBSCAN Score: 0.2710561739105903\n",
      "Mean Shift Clustering Score: 0.26251989513072405\n",
      "Gaussian Mixture Models (GMM) Score: 0.2763788352221043\n",
      "\n",
      "Dimensionality Reduction Techniques:\n",
      "PCA + K-means Score: 0.47302785983313894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE + K-means Score: 0.35715732\n",
      "\n",
      "One-Class SVM:\n",
      "One-Class SVM Score: 0.13126448677130953\n",
      "\n",
      "Kernel Density Estimation (KDE):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "maximum supported dimension for an ndarray is 32, found 10670",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3432\\2717675147.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mkde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"KDE Score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_kde.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: maximum supported dimension for an ndarray is 32, found 10670"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Define preprocessing steps for numeric and non-numeric columns\n",
    "numeric_features = data[columns_of_interest].select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = data[columns_of_interest].select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Choose a different strategy for non-numeric data\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing and imputation\n",
    "processed_data = preprocessor.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Initialize clustering algorithms and dimensionality reduction techniques\n",
    "algorithms = {\n",
    "    'K-means': KMeans(n_clusters=3),\n",
    "    'Hierarchical Clustering': AgglomerativeClustering(n_clusters=3),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Mean Shift Clustering': MeanShift(),\n",
    "    'Gaussian Mixture Models (GMM)': GaussianMixture(n_components=3),\n",
    "}\n",
    "\n",
    "dimensionality_reduction = {\n",
    "    'PCA': PCA(n_components=2),\n",
    "    't-SNE': TSNE(n_components=2),\n",
    "}\n",
    "\n",
    "# Evaluate clustering algorithms\n",
    "print(\"Clustering Algorithms:\")\n",
    "for alg_name, alg in algorithms.items():\n",
    "    alg.fit(processed_data)\n",
    "    if hasattr(alg, 'labels_'):\n",
    "        labels = alg.labels_\n",
    "    else:\n",
    "        labels = alg.predict(processed_data)\n",
    "    score = silhouette_score(processed_data, labels, metric='euclidean')\n",
    "    print(f\"{alg_name} Score:\", score)\n",
    "\n",
    "# Evaluate dimensionality reduction techniques\n",
    "print(\"\\nDimensionality Reduction Techniques:\")\n",
    "for dim_red_name, dim_red in dimensionality_reduction.items():\n",
    "    reduced_data = dim_red.fit_transform(processed_data)\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(reduced_data)\n",
    "    score = silhouette_score(reduced_data, kmeans.labels_, metric='euclidean')\n",
    "    print(f\"{dim_red_name} + K-means Score:\", score)\n",
    "\n",
    "# One-Class SVM\n",
    "print(\"\\nOne-Class SVM:\")\n",
    "oneclass_svm = OneClassSVM(gamma='auto')\n",
    "oneclass_svm.fit(processed_data)\n",
    "print(\"One-Class SVM Score:\", silhouette_score(processed_data, oneclass_svm.predict(processed_data)))\n",
    "\n",
    "# KDE\n",
    "print(\"\\nKernel Density Estimation (KDE):\")\n",
    "kde = KernelDensity()\n",
    "kde.fit(processed_data)\n",
    "print(\"KDE Score:\", silhouette_score(processed_data, kde.sample(processed_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbf04197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Algorithms:\n",
      "K-means Score: 0.42904010086077227\n",
      "Hierarchical Clustering Score: 0.3132061510996466\n",
      "DBSCAN Score: 0.2710561739105903\n",
      "Mean Shift Clustering Score: 0.26251989513072405\n",
      "Gaussian Mixture Models (GMM) Score: 0.2763788352221043\n",
      "\n",
      "Dimensionality Reduction Techniques:\n",
      "PCA + K-means Score: 0.47304642748656206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE + K-means Score: 0.365226\n",
      "\n",
      "One-Class SVM:\n",
      "One-Class SVM Score: 0.13126448677130953\n",
      "\n",
      "Kernel Density Estimation (KDE) after Dimensionality Reduction:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "maximum supported dimension for an ndarray is 32, found 10670",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3432\\3615066224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mkde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_reduced_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"KDE Score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_reduced_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_reduced_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_kde.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n_samples, random_state)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: maximum supported dimension for an ndarray is 32, found 10670"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Define preprocessing steps for numeric and non-numeric columns\n",
    "numeric_features = data[columns_of_interest].select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = data[columns_of_interest].select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Choose a different strategy for non-numeric data\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing and imputation\n",
    "processed_data = preprocessor.fit_transform(data[columns_of_interest])\n",
    "\n",
    "# Initialize clustering algorithms and dimensionality reduction techniques\n",
    "algorithms = {\n",
    "    'K-means': KMeans(n_clusters=3),\n",
    "    'Hierarchical Clustering': AgglomerativeClustering(n_clusters=3),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Mean Shift Clustering': MeanShift(),\n",
    "    'Gaussian Mixture Models (GMM)': GaussianMixture(n_components=3),\n",
    "}\n",
    "\n",
    "dimensionality_reduction = {\n",
    "    'PCA': PCA(n_components=2),\n",
    "    't-SNE': TSNE(n_components=2),\n",
    "}\n",
    "\n",
    "# Evaluate clustering algorithms\n",
    "print(\"Clustering Algorithms:\")\n",
    "for alg_name, alg in algorithms.items():\n",
    "    alg.fit(processed_data)\n",
    "    if hasattr(alg, 'labels_'):\n",
    "        labels = alg.labels_\n",
    "    else:\n",
    "        labels = alg.predict(processed_data)\n",
    "    score = silhouette_score(processed_data, labels, metric='euclidean')\n",
    "    print(f\"{alg_name} Score:\", score)\n",
    "\n",
    "# Evaluate dimensionality reduction techniques\n",
    "print(\"\\nDimensionality Reduction Techniques:\")\n",
    "for dim_red_name, dim_red in dimensionality_reduction.items():\n",
    "    reduced_data = dim_red.fit_transform(processed_data)\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(reduced_data)\n",
    "    score = silhouette_score(reduced_data, kmeans.labels_, metric='euclidean')\n",
    "    print(f\"{dim_red_name} + K-means Score:\", score)\n",
    "\n",
    "# One-Class SVM\n",
    "print(\"\\nOne-Class SVM:\")\n",
    "oneclass_svm = OneClassSVM(gamma='auto')\n",
    "oneclass_svm.fit(processed_data)\n",
    "print(\"One-Class SVM Score:\", silhouette_score(processed_data, oneclass_svm.predict(processed_data)))\n",
    "\n",
    "# Kernel Density Estimation (KDE) after dimensionality reduction\n",
    "print(\"\\nKernel Density Estimation (KDE) after Dimensionality Reduction:\")\n",
    "pca_reduced_data = PCA(n_components=2).fit_transform(processed_data)\n",
    "kde = KernelDensity()\n",
    "kde.fit(pca_reduced_data)\n",
    "print(\"KDE Score:\", silhouette_score(pca_reduced_data, kde.sample(pca_reduced_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2fe7395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means Score: 0.46114702789401185\n",
      "Hierarchical Clustering Score: 0.42596289756187583\n",
      "DBSCAN Score: 0.7427003845774999\n",
      "The following algorithms are sensitive:\n",
      "DBSCAN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "# Filter out non-numeric columns\n",
    "numeric_columns = data[columns_of_interest].select_dtypes(include=['number']).columns\n",
    "data_filtered = data[numeric_columns]\n",
    "\n",
    "# Preprocessing: Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_filtered = imputer.fit_transform(data_filtered)\n",
    "\n",
    "# Preprocessing: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_filtered)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement and evaluate clustering algorithms\n",
    "algorithms = {\n",
    "    'K-means': KMeans(n_clusters=2),\n",
    "    'Hierarchical Clustering': AgglomerativeClustering(n_clusters=2),\n",
    "    'DBSCAN': DBSCAN(),\n",
    "}\n",
    "\n",
    "# Store silhouette scores for each algorithm\n",
    "silhouette_scores = {}\n",
    "\n",
    "for alg_name, alg in algorithms.items():\n",
    "    alg.fit(X_train)\n",
    "    score = silhouette_score(X_train, alg.labels_, metric='euclidean')\n",
    "    silhouette_scores[alg_name] = score\n",
    "    print(f\"{alg_name} Score:\", score)\n",
    "\n",
    "# Determine sensitivity\n",
    "sensitive_algorithms = [alg_name for alg_name, score in silhouette_scores.items() if score > 0.5]\n",
    "\n",
    "if sensitive_algorithms:\n",
    "    print(\"The following algorithms are sensitive:\")\n",
    "    for alg_name in sensitive_algorithms:\n",
    "        print(alg_name)\n",
    "else:\n",
    "    print(\"No sensitive algorithms found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6517af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is sensitive.\n",
      "Sensitive algorithms: ['DBSCAN']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def check_sensitivity(data):\n",
    "    # Select columns of interest\n",
    "    columns_of_interest = ['PIMOF', 'MIS', 'MILEAGE (KM)']\n",
    "\n",
    "    # Filter out non-numeric columns\n",
    "    numeric_columns = data[columns_of_interest].select_dtypes(include=['number']).columns\n",
    "    data_filtered = data[numeric_columns]\n",
    "\n",
    "    # Preprocessing: Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_filtered = imputer.fit_transform(data_filtered)\n",
    "\n",
    "    # Preprocessing: Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_filtered)\n",
    "\n",
    "    # Implement and evaluate clustering algorithms\n",
    "    algorithms = {\n",
    "        'K-means': KMeans(n_clusters=2),\n",
    "        'Hierarchical Clustering': AgglomerativeClustering(n_clusters=2),\n",
    "        'DBSCAN': DBSCAN(),\n",
    "    }\n",
    "\n",
    "    # Store silhouette scores for each algorithm\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for alg_name, alg in algorithms.items():\n",
    "        alg.fit(data_scaled)\n",
    "        score = silhouette_score(data_scaled, alg.labels_, metric='euclidean')\n",
    "        silhouette_scores[alg_name] = score\n",
    "\n",
    "    # Determine sensitivity\n",
    "    sensitive_algorithms = [alg_name for alg_name, score in silhouette_scores.items() if score > 0.5]\n",
    "\n",
    "    if sensitive_algorithms:\n",
    "        print(\"The data is sensitive.\")\n",
    "        print(\"Sensitive algorithms:\", sensitive_algorithms)\n",
    "    else:\n",
    "        print(\"The data is not sensitive.\")\n",
    "\n",
    "\n",
    "data = pd.read_excel('Dummy Customer Compliant Data.xlsx', header=1)\n",
    "check_sensitivity(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8189b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is not sensitive.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def check_sensitivity(data):\n",
    "    # Preprocessing: Handle missing values\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "\n",
    "    # Preprocessing: Encode non-numeric values\n",
    "    label_encoders = {}\n",
    "    for col_index, col_name in enumerate(data.columns):\n",
    "        # Convert column to string\n",
    "        col_data = data_imputed[:, col_index].astype(str)\n",
    "        # Encode non-numeric values\n",
    "        label_encoders[col_name] = LabelEncoder()\n",
    "        data_imputed[:, col_index] = label_encoders[col_name].fit_transform(col_data)\n",
    "\n",
    "    # Preprocessing: Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_imputed)\n",
    "\n",
    "    # Implement and evaluate clustering algorithms\n",
    "    algorithms = {\n",
    "        'K-means': KMeans(n_clusters=2),\n",
    "        'Hierarchical Clustering': AgglomerativeClustering(n_clusters=2),\n",
    "        'DBSCAN': DBSCAN(),\n",
    "    }\n",
    "\n",
    "    # Store silhouette scores for each algorithm\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for alg_name, alg in algorithms.items():\n",
    "        alg.fit(data_scaled)\n",
    "        score = silhouette_score(data_scaled, alg.labels_, metric='euclidean')\n",
    "        silhouette_scores[alg_name] = score\n",
    "\n",
    "    # Determine sensitivity\n",
    "    sensitive_algorithms = [alg_name for alg_name, score in silhouette_scores.items() if score > 0.5]\n",
    "\n",
    "    if sensitive_algorithms:\n",
    "        print(\"The data is sensitive.\")\n",
    "        print(\"Sensitive algorithms:\", sensitive_algorithms)\n",
    "    else:\n",
    "        print(\"The data is not sensitive.\")\n",
    "\n",
    "\n",
    "file_path = 'Dummy Customer Compliant Data.xlsx'\n",
    "data = pd.read_excel(file_path, header=1)\n",
    "check_sensitivity(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f3333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
